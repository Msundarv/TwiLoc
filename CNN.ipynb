{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiRZz5pRdiT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding,Conv1D,GlobalMaxPooling1D,Dense,MaxPooling1D,Flatten,Input\n",
        "from keras.models import Model\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRiPRnOwdlJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train,dev and test datasets\n",
        "\n",
        "def loadData(filepath):\n",
        "  data = pd.read_csv(filepath)\n",
        "  # Currently working on region level predictor\n",
        "  data.drop(['lat','long','state'],axis=1,inplace=True)\n",
        "  # Filter and remove anomaly data\n",
        "  data = data[data.region != 'MX']  # Remove 'Mexico' records - Due to Reverse geocoding\n",
        "  data = data[data.region != 'CA']  # Remove 'Canada' records - Due to Reverse geocoding\n",
        "  data.dropna(inplace=True)\n",
        "  data.reset_index(drop=True,inplace=True)\n",
        "  return data\n",
        " \n",
        "# train file and dev file has columns tweet, lat, long, state, region\n",
        "# test file has columns uid (userid), tweet, lat, long, state, region\n",
        "filename1= #TRAIN_PATH\n",
        "filename2= #TEST_PATH\n",
        "filename4= #DEV_PATH\n",
        "glove_file = #EMBEDDING_FILE_PATH\n",
        "\n",
        "train_df = pd.read_csv(filename1)\n",
        "dev_df = pd.read_csv(filename4)\n",
        "test_df = pd.read_csv(filename2)\n",
        "\n",
        "frames = [train_df,dev_df]\n",
        "merge_train_df = pd.concat(frames)\n",
        "#print(merge_train_df.head())\n",
        "\n",
        "test = loadData(#TEST_PATH)\n",
        "tweet_test = new = test.drop('uid', axis=1)\n",
        "user_test = test.drop('tweet', axis=1).drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCCgl4KWeIea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label Preparation\n",
        "\n",
        "train_Y = merge_train_df['region']\n",
        "dev_Y = dev_df['region']\n",
        "test_Y = test_df['region']\n",
        "#print(train_Y.head())\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_Y)\n",
        "train_Y = encoder.transform(train_Y)\n",
        "dev_Y = encoder.transform(dev_Y)\n",
        "test_Y = encoder.transform(test_Y)\n",
        "\n",
        "num_classes = np.max(train_Y) + 1\n",
        "#print(num_classes)\n",
        "\n",
        "train_Y = utils.to_categorical(train_Y, num_classes)\n",
        "dev_Y = utils.to_categorical(dev_Y, num_classes)\n",
        "test_Y = utils.to_categorical(test_Y, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNlaIk0ReoTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preperation\n",
        "\n",
        "\"\"\"\n",
        "preprocess-twitter.py\n",
        "\n",
        "Script for preprocessing tweets by Romain Paulus\n",
        "with small modifications by Jeffrey Pennington\n",
        "with translation to Python by Motoki Wu\n",
        "\n",
        "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
        "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import regex as re\n",
        "\n",
        "FLAGS = re.MULTILINE | re.DOTALL\n",
        "\n",
        "def hashtag(text):\n",
        "    text = text.group()\n",
        "    hashtag_body = text[1:]\n",
        "    if hashtag_body.isupper():\n",
        "        result = \" {} \".format(hashtag_body.lower())\n",
        "    else:\n",
        "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
        "    return result\n",
        "\n",
        "def allcaps(text):\n",
        "    text = text.group()\n",
        "    return text.lower() + \" <allcaps>\"\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    # Different regex parts for smiley faces\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "\n",
        "    # function so code less repetitive\n",
        "    def re_sub(pattern, repl):\n",
        "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "\n",
        "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
        "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
        "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
        "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
        "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
        "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
        "    text = re_sub(r\"/\",\" / \")\n",
        "    text = re_sub(r\"<3\",\"<heart>\")\n",
        "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
        "    text = re_sub(r\"#\\S+\", hashtag)\n",
        "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
        "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
        "\n",
        "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
        "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
        "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
        "\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ilkCj6eWyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "x_train = merge_train_df[\"tweet\"]\n",
        "x_test = test_df[\"tweet\"]\n",
        "x_dev = dev_df[\"tweet\"]\n",
        "\n",
        "x_train = x_train.apply(tokenize)\n",
        "x_dev = x_dev.apply(tokenize)\n",
        "x_test = x_test.apply(tokenize)\n",
        "#print(train_X.head())\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "#print(len(tokenizer.word_index))\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_dev = tokenizer.texts_to_sequences(x_dev)\n",
        "#print(x_train[0])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "#print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihwo029RfrkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create embedding matrix\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9itrYQ38fFXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorization\n",
        "\n",
        "maxlen = 25\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_dev = pad_sequences(x_dev, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
        "#print(x_train[0, :])\n",
        "\n",
        "embedding_dim = 200\n",
        "embedding_matrix = create_embedding_matrix(glove_file,tokenizer.word_index, embedding_dim)\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "vocab_coverage = nonzero_elements / vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nfctWGuf9qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN Model\n",
        "\n",
        "batch_size = 40000\n",
        "epochs = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],input_length=maxlen,trainable=True))\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 4, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kdZodnmtAHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparamter Optimization\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen,layers):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],input_length=maxlen,trainable=False))\n",
        "    model.add(Conv1D(num_filters, kernel_size, activation='relu',padding='same'))\n",
        "    model.add(MaxPooling1D(padding=\"same\"))\n",
        "    for i in range(layers):\n",
        "      model.add(Conv1D(num_filters, kernel_size, activation='relu',padding='same'))\n",
        "      model.add(MaxPooling1D(padding='same'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "  \n",
        "  param_grid = dict(num_filters=[64],\n",
        "                  kernel_size=[5],\n",
        "                  vocab_size=[vocab_size], \n",
        "                  embedding_dim=[embedding_dim],\n",
        "                  maxlen=[10],\n",
        "                  layers=[3,4,5],\n",
        "                  )\n",
        "  \n",
        "  model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=batch_size,\n",
        "                            verbose=1)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,cv=3, verbose=1, n_iter=17)\n",
        "grid_result = grid.fit(x_train, train_Y)\n",
        "\n",
        "test_accuracy = grid.score(x_test, test_Y)\n",
        "\n",
        "output_file = # OUTPUT_FILE_PATH_FOR_HYPER_PARAMS\n",
        "with open(output_file, 'a') as f:\n",
        "        s = ('Best Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)\n",
        "print(grid_result.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I4m5yEXhFTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and Predict CNN model\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "filepath= # PATH_TO_SAVE_MODEL\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "callbacks_list = [checkpoint,es]\n",
        "\n",
        "history = model.fit(x_train, train_Y,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, test_Y),\n",
        "                    callbacks=callbacks_list)\n",
        "\n",
        "score = model.evaluate(x_test, test_Y,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSU2e57thZ4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training and validation loss and accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E1Mps_jhxBO",
        "colab_type": "code",
        "outputId": "3184baf9-2d9d-4f0c-dbdf-d1cd14a31f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "# Evaluation of best CNN model\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],input_length=maxlen,trainable=True))\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 4, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(64, 5, activation='relu',padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(\"PATH_TO_BEST_MODEL\")\n",
        "\n",
        "cnn_loss, cnn_accuracy = model.evaluate(x_test, test_Y, verbose=False)\n",
        "print(\"CNN 1D Best Tweet Level Test Accuracy:  {:.2f}\".format(cnn_accuracy*100))\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "#print(y_pred.shape)\n",
        "\n",
        "#print(y_pred[0])\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "#print(y_pred[0])\n",
        "#print(y_pred.shape)\n",
        "\n",
        "test['ypred'] = y_pred\n",
        "user_pred = test.drop(['tweet','region'],axis=1)\n",
        "# print(user_pred.head(5))\n",
        "result = user_pred.groupby(['uid'])['ypred'].agg(lambda x:x.value_counts().index[0])\n",
        "#print(result.head(10))\n",
        "true = user_test.sort_values(by=['uid']).replace({ 'midwest': 0, 'northeast': 1, 'south': 2, 'west' : 3 })\n",
        "#print(true.head(10))\n",
        "\n",
        "# print(result[0])\n",
        "final_accuracy = accuracy_score(true['region'], result)\n",
        "print(\"CNN 1D Final Accuracy:  {:.2f}\".format(final_accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 25, 200)           20210800  \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 25, 64)            64064     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 25, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 12, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 12, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 12, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 6, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 6, 64)             20544     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 64)             0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 3, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 3, 64)             20544     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 3, 64)             0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 1, 64)             20544     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 44        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 20,353,638\n",
            "Trainable params: 20,353,638\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CNN 1D Best Tweet Level Test Accuracy:  46.97\n",
            "CNN 1D Final Accuracy:  57.43\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
