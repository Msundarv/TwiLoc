{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOnGmAePvBGK"
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "RSED6JRO7BEH",
    "outputId": "67bdb66b-1f5e-4ff6-a3ac-7f8e4aef2cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (292226, 2)\n",
      "Dev shape: (74079, 2)\n",
      "Test shape: (74382, 3)\n",
      "Tweet Test shape: (74382, 2)\n",
      "User Test shape: (1858, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load Training, Dev and Test Data\n",
    "\n",
    "def loadData(filepath):\n",
    "  data = pd.read_csv(filepath)\n",
    "  # Currently working on region level predictor\n",
    "  data.drop(['lat','long','state'],axis=1,inplace=True)\n",
    "  # Filter and remove anomaly data\n",
    "  data = data[data.region != 'MX']  # Remove 'Mexico' records - Due to Reverse geocoding\n",
    "  data = data[data.region != 'CA']  # Remove 'Canada' records - Due to Reverse geocoding\n",
    "  data.dropna(inplace=True)\n",
    "  data.reset_index(drop=True,inplace=True)\n",
    "  return data\n",
    "\n",
    "# train file and dev file has columns tweet, lat, long, state, region\n",
    "# test file has columns uid (userid), tweet, lat, long, state, region\n",
    "train = loadData(\"Path to train .csv file\")\n",
    "dev = loadData(\"Path to dev .csv file\")\n",
    "test = loadData(\"Path to test .csv file\")\n",
    "tweet_test = test.drop('uid', axis=1)  # Tweet level test\n",
    "user_test = test.drop('tweet', axis=1).drop_duplicates()  # User level test\n",
    "\n",
    "# print(train.head(5))\n",
    "# print(train.iloc[0])\n",
    "# dev.region.unique()\n",
    "# train.info()\n",
    "print(\"Train shape: \"+ str(train.shape))\n",
    "print(\"Dev shape: \"+ str(dev.shape))\n",
    "print(\"Test shape: \"+ str(test.shape))\n",
    "print(\"Tweet Test shape: \"+ str(tweet_test.shape))\n",
    "print(\"User Test shape: \"+ str(user_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8WuCHvRSNO9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess-twitter.py\n",
    "\n",
    "Script for preprocessing tweets by Romain Paulus\n",
    "with small modifications by Jeffrey Pennington\n",
    "with translation to Python by Motoki Wu\n",
    "\n",
    "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
    "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Ody5PVriu5Lt",
    "outputId": "c330acae-5367-4934-ad5d-ccf0ded46bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (292226, 2)\n",
      "Dev shape: (74079, 2)\n",
      "Test shape: (74382, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Tweet cleaning\n",
    "pat1 = r'@[A-Za-z0-9_]+'  # To remove '@' mentions\n",
    "pat2 = r'https?://[^ ]+'  # To remove web links starting with http\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'  # To remove web links starting without http\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    # stripped = re.sub(combined_pat, '', text)\n",
    "    # stripped = re.sub(www_pat, '', stripped)\n",
    "    return tokenize(text)\n",
    "  \n",
    "# print(tweet_cleaner(\" www.msundarv.com don't First batting also dhik dhik, chasing also dhik dhik! Finger nails, toe nails, no bias! üò∑ #Yellove #WhistlePodu #CSKvRR ü¶Åüíõ\"))\n",
    "\n",
    "train.tweet = train.tweet.apply(tweet_cleaner)\n",
    "# print(train.head(5))\n",
    "# print(train.iloc[0])\n",
    "dev.tweet = dev.tweet.apply(tweet_cleaner)\n",
    "tweet_test.tweet = tweet_test.tweet.apply(tweet_cleaner)\n",
    "\n",
    "print(\"Train shape: \"+ str(train.shape))\n",
    "print(\"Dev shape: \"+ str(dev.shape))\n",
    "print(\"Test shape: \"+ str(tweet_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwWIthhgZYmn"
   },
   "outputs": [],
   "source": [
    "# Create Glove Twitter Word Embedding Matrix\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "rcFan4t81BB9",
    "outputId": "78a2ea22-e318-4bac-8e32-00b04d71209c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 101054\n",
      "X_Train shape: (292226, 25)\n",
      "X_Dev shape: (74079, 25)\n",
      "X_Test shape: (74382, 25)\n",
      "Vocab covered:  60.37\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tweet_tokenizer = Tokenizer()\n",
    "tweet_tokenizer.fit_on_texts(train.tweet)\n",
    "tweet_vocab_size = len(tweet_tokenizer.word_index) + 1\n",
    "print(\"Vocab size: \"+ str(tweet_vocab_size))\n",
    "\n",
    "X_train = tweet_tokenizer.texts_to_sequences(train.tweet)\n",
    "X_dev = tweet_tokenizer.texts_to_sequences(dev.tweet)\n",
    "X_test = tweet_tokenizer.texts_to_sequences(tweet_test.tweet)\n",
    "\n",
    "tweet_len = 25\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=tweet_len)\n",
    "X_dev = pad_sequences(X_dev, padding='post', maxlen=tweet_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=tweet_len)\n",
    "\n",
    "print(\"X_Train shape: \"+ str(X_train.shape))\n",
    "print(\"X_Dev shape: \"+ str(X_dev.shape))\n",
    "print(\"X_Test shape: \"+ str(X_test.shape))\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('Path to glove.twitter.27B.100d.txt', tweet_tokenizer.word_index, embedding_dim)\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(\"Vocab covered:  {:.2f}\".format((nonzero_elements / tweet_vocab_size)*100))\n",
    "\n",
    "y_train = to_categorical(train.region.replace({ 'midwest': 0, 'northeast': 1, 'south': 2, 'west' : 3 }), num_classes=None)\n",
    "y_dev = to_categorical(dev.region.replace({ 'midwest': 0, 'northeast': 1, 'south': 2, 'west' : 3 }), num_classes=None)\n",
    "y_test = to_categorical(tweet_test.region.replace({ 'midwest': 0, 'northeast': 1, 'south': 2, 'west' : 3 }), num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GF_eBmxfXsaR"
   },
   "outputs": [],
   "source": [
    "# Define GRU Model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "def create_model(no_of_units, no_of_layers):\n",
    "  print(\"[No. of units, No. of layers]: \"+ str([no_of_units, no_of_layers]))\n",
    "  model = Sequential()\n",
    "  model.add(layers.Embedding(tweet_vocab_size, embedding_dim, \n",
    "                             weights=[embedding_matrix],\n",
    "                             input_length = tweet_len,\n",
    "                             trainable = True))\n",
    "  \n",
    "  for i in range(no_of_layers):\n",
    "    if i == (no_of_layers - 1):\n",
    "      model.add(layers.Bidirectional(layers.GRU(no_of_units)))\n",
    "    else:\n",
    "      model.add(layers.Bidirectional(layers.GRU(no_of_units, return_sequences=True)))\n",
    "  model.add(layers.Dense(no_of_units, activation='relu'))\n",
    "  model.add(layers.GaussianNoise(0.2)) \n",
    "  model.add(layers.Dense(4, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pMYPlpUKaaI"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters Optimization\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = dict(no_of_units=[10, 15, 20, 25, 30], no_of_layers = [2, 3, 4])\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=3, batch_size=128, verbose=1)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=3, verbose=1, n_iter=7)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Grid Score:  {:.2f}\".format(grid_result.best_score_*100))\n",
    "print(\"Best Grid params: \"+ str(grid_result.best_params_))\n",
    "\n",
    "gru_grid_accuracy = grid.score(X_test, y_test)\n",
    "print(\"GRU Grid Test Accuracy:  {:.2f}\".format(gru_grid_accuracy*100))\n",
    "\n",
    "# Best params {'no_of_units': 10, 'no_of_layers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhHTXPeodLvW"
   },
   "outputs": [],
   "source": [
    "# Train and predict using GRU\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = create_model(10,3) \n",
    "\n",
    "# Save the checkpoint\n",
    "ckptfilepath = \"Path to .hdf5 checkpopint file\"\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "checkpoint = ModelCheckpoint(ckptfilepath,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=100, validation_data=(X_dev, y_dev), batch_size=10000, callbacks=[checkpoint, early_stopping_monitor])\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_dev, y_dev), batch_size=128, callbacks=[checkpoint])\n",
    "\n",
    "gru_loss, gru_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"GRU Test Accuracy:  {:.2f}\".format(gru_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oN2IKxrUYrxD"
   },
   "outputs": [],
   "source": [
    " # Plot training loss values and metrics values \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "Na-MpncK3k7U",
    "outputId": "4613a5da-7fc6-441c-86e1-f88af34e2c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[No. of units, No. of layers]: [10, 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 100)           10105400  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 25, 20)            6660      \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 25, 20)            1860      \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 20)                1860      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 10,116,034\n",
      "Trainable params: 10,116,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "GRU Best Tweet level Test Accuracy:  47.06\n",
      "GRU Final Accuracy:  56.35\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best fit GRU\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bestmodelfilepath = \"Path to best model .hdf5 file\"\n",
    "\n",
    "model = create_model(10,3)\n",
    "model.load_weights(bestmodelfilepath)\n",
    "\n",
    "gru_loss, gru_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"GRU Best Tweet level Test Accuracy:  {:.2f}\".format(gru_accuracy*100))\n",
    "\n",
    "\n",
    "# Predict tweet level regions using trained model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "# print(y_pred.shape)\n",
    "\n",
    "# Predict user level region\n",
    "test['ypred'] = y_pred\n",
    "user_pred = test.drop(['tweet','region'],axis=1)\n",
    "pred = user_pred.groupby(['uid'])['ypred'].agg(lambda x:x.value_counts().index[0])\n",
    "# print(pred.head(10))\n",
    "\n",
    "# Compare with true values\n",
    "true = user_test.sort_values(by=['uid']).replace({ 'midwest': 0, 'northeast': 1, 'south': 2, 'west' : 3 })\n",
    "# print(true.head(10))\n",
    "\n",
    "final_acc = accuracy_score(true['region'], pred)\n",
    "print(\"GRU Final Accuracy:  {:.2f}\".format(final_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLi0ueA1Av-m"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GRU.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
